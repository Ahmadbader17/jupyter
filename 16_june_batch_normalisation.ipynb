{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f7ac8de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 2s 817us/step - loss: 0.2730 - accuracy: 0.9221 - val_loss: 0.1459 - val_accuracy: 0.9567\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 1s 750us/step - loss: 0.1230 - accuracy: 0.9621 - val_loss: 0.1117 - val_accuracy: 0.9647\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 1s 753us/step - loss: 0.0901 - accuracy: 0.9714 - val_loss: 0.0976 - val_accuracy: 0.9687\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 1s 749us/step - loss: 0.0705 - accuracy: 0.9777 - val_loss: 0.0895 - val_accuracy: 0.9730\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 1s 749us/step - loss: 0.0577 - accuracy: 0.9818 - val_loss: 0.0893 - val_accuracy: 0.9726\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 1s 747us/step - loss: 0.0485 - accuracy: 0.9843 - val_loss: 0.0964 - val_accuracy: 0.9727\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 1s 748us/step - loss: 0.0427 - accuracy: 0.9861 - val_loss: 0.0973 - val_accuracy: 0.9705\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 1s 748us/step - loss: 0.0348 - accuracy: 0.9883 - val_loss: 0.0865 - val_accuracy: 0.9747\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 1s 746us/step - loss: 0.0305 - accuracy: 0.9899 - val_loss: 0.0982 - val_accuracy: 0.9733\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 1s 747us/step - loss: 0.0274 - accuracy: 0.9911 - val_loss: 0.0968 - val_accuracy: 0.9751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 2s 927us/step - loss: 0.2798 - accuracy: 0.9175 - val_loss: 0.1438 - val_accuracy: 0.9551\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 2s 875us/step - loss: 0.1430 - accuracy: 0.9564 - val_loss: 0.1099 - val_accuracy: 0.9668\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 2s 871us/step - loss: 0.1115 - accuracy: 0.9657 - val_loss: 0.1035 - val_accuracy: 0.9684\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 2s 867us/step - loss: 0.0939 - accuracy: 0.9706 - val_loss: 0.0815 - val_accuracy: 0.9738\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 2s 867us/step - loss: 0.0809 - accuracy: 0.9744 - val_loss: 0.0857 - val_accuracy: 0.9746\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 2s 869us/step - loss: 0.0754 - accuracy: 0.9762 - val_loss: 0.0826 - val_accuracy: 0.9735\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 2s 898us/step - loss: 0.0671 - accuracy: 0.9785 - val_loss: 0.0847 - val_accuracy: 0.9739\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 2s 870us/step - loss: 0.0605 - accuracy: 0.9802 - val_loss: 0.0737 - val_accuracy: 0.9781\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 2s 868us/step - loss: 0.0566 - accuracy: 0.9817 - val_loss: 0.0778 - val_accuracy: 0.9770\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 2s 870us/step - loss: 0.0549 - accuracy: 0.9821 - val_loss: 0.0829 - val_accuracy: 0.9764\n",
      "313/313 [==============================] - 0s 471us/step - loss: 0.0968 - accuracy: 0.9751\n",
      "Model without batch normalization - Accuracy: 0.9750999808311462\n",
      "313/313 [==============================] - 0s 525us/step - loss: 0.0829 - accuracy: 0.9764\n",
      "Model with batch normalization - Accuracy: 0.9764000177383423\n",
      "Batch normalization improves model performance by: 0.001300036907196045\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Step 1: Preprocess the dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Step 2: Implement a simple feedforward neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Step 3: Train the neural network without batch normalization\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train.reshape(-1, 784), y_train, epochs=10, batch_size=32, validation_data=(x_test.reshape(-1, 784), y_test))\n",
    "\n",
    "# Step 4: Implement batch normalization layers and train the model again\n",
    "model_with_bn = Sequential()\n",
    "model_with_bn.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "model_with_bn.add(BatchNormalization())\n",
    "model_with_bn.add(Dense(64, activation='relu'))\n",
    "model_with_bn.add(BatchNormalization())\n",
    "model_with_bn.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_with_bn.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "model_with_bn.fit(x_train.reshape(-1, 784), y_train, epochs=10, batch_size=32, validation_data=(x_test.reshape(-1, 784), y_test))\n",
    "\n",
    "# Step 5: Compare the training and validation performance\n",
    "_, accuracy = model.evaluate(x_test.reshape(-1, 784), y_test)\n",
    "print(\"Model without batch normalization - Accuracy:\", accuracy)\n",
    "\n",
    "_, accuracy_bn = model_with_bn.evaluate(x_test.reshape(-1, 784), y_test)\n",
    "print(\"Model with batch normalization - Accuracy:\", accuracy_bn)\n",
    "\n",
    "# Step 6: Discuss the impact of batch normalization\n",
    "print(\"Batch normalization improves model performance by:\", accuracy_bn - accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77b29ab",
   "metadata": {},
   "source": [
    "# Experiment with different batch sizes and observe the effect on the training dynamics and model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d50fa7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3750/3750 [==============================] - 3s 757us/step - loss: 0.2466 - accuracy: 0.9276 - val_loss: 0.1302 - val_accuracy: 0.9606\n",
      "Epoch 2/10\n",
      "3750/3750 [==============================] - 3s 736us/step - loss: 0.1138 - accuracy: 0.9659 - val_loss: 0.1052 - val_accuracy: 0.9669\n",
      "Epoch 3/10\n",
      "3750/3750 [==============================] - 3s 701us/step - loss: 0.0837 - accuracy: 0.9738 - val_loss: 0.1018 - val_accuracy: 0.9681\n",
      "Epoch 4/10\n",
      "3750/3750 [==============================] - 3s 702us/step - loss: 0.0657 - accuracy: 0.9796 - val_loss: 0.0842 - val_accuracy: 0.9749\n",
      "Epoch 5/10\n",
      "3750/3750 [==============================] - 3s 706us/step - loss: 0.0548 - accuracy: 0.9824 - val_loss: 0.0904 - val_accuracy: 0.9741\n",
      "Epoch 6/10\n",
      "3750/3750 [==============================] - 3s 712us/step - loss: 0.0454 - accuracy: 0.9857 - val_loss: 0.1023 - val_accuracy: 0.9714\n",
      "Epoch 7/10\n",
      "3750/3750 [==============================] - 3s 705us/step - loss: 0.0395 - accuracy: 0.9868 - val_loss: 0.1032 - val_accuracy: 0.9733\n",
      "Epoch 8/10\n",
      "3750/3750 [==============================] - 3s 704us/step - loss: 0.0354 - accuracy: 0.9883 - val_loss: 0.1020 - val_accuracy: 0.9767\n",
      "Epoch 9/10\n",
      "3750/3750 [==============================] - 3s 708us/step - loss: 0.0297 - accuracy: 0.9903 - val_loss: 0.0969 - val_accuracy: 0.9767\n",
      "Epoch 10/10\n",
      "3750/3750 [==============================] - 3s 720us/step - loss: 0.0283 - accuracy: 0.9902 - val_loss: 0.1064 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 2s 854us/step - loss: 0.2794 - accuracy: 0.9201 - val_loss: 0.1551 - val_accuracy: 0.9550\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 1s 759us/step - loss: 0.1213 - accuracy: 0.9639 - val_loss: 0.1177 - val_accuracy: 0.9631\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 1s 789us/step - loss: 0.0885 - accuracy: 0.9733 - val_loss: 0.0956 - val_accuracy: 0.9705\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 1s 757us/step - loss: 0.0704 - accuracy: 0.9780 - val_loss: 0.0937 - val_accuracy: 0.9708\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 1s 765us/step - loss: 0.0566 - accuracy: 0.9811 - val_loss: 0.0882 - val_accuracy: 0.9750\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 1s 756us/step - loss: 0.0490 - accuracy: 0.9840 - val_loss: 0.0909 - val_accuracy: 0.9732\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 1s 767us/step - loss: 0.0403 - accuracy: 0.9870 - val_loss: 0.0971 - val_accuracy: 0.9705\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 1s 763us/step - loss: 0.0340 - accuracy: 0.9887 - val_loss: 0.0886 - val_accuracy: 0.9768\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 1s 763us/step - loss: 0.0298 - accuracy: 0.9897 - val_loss: 0.0869 - val_accuracy: 0.9752\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 1s 793us/step - loss: 0.0269 - accuracy: 0.9912 - val_loss: 0.0900 - val_accuracy: 0.9757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 1s 984us/step - loss: 0.3198 - accuracy: 0.9098 - val_loss: 0.1627 - val_accuracy: 0.9502\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 1s 898us/step - loss: 0.1430 - accuracy: 0.9577 - val_loss: 0.1154 - val_accuracy: 0.9645\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 1s 895us/step - loss: 0.1060 - accuracy: 0.9685 - val_loss: 0.1069 - val_accuracy: 0.9674\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 1s 936us/step - loss: 0.0848 - accuracy: 0.9742 - val_loss: 0.1012 - val_accuracy: 0.9665\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 919us/step - loss: 0.0705 - accuracy: 0.9783 - val_loss: 0.0989 - val_accuracy: 0.9679\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 905us/step - loss: 0.0586 - accuracy: 0.9824 - val_loss: 0.0909 - val_accuracy: 0.9722\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 898us/step - loss: 0.0509 - accuracy: 0.9845 - val_loss: 0.0973 - val_accuracy: 0.9712\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 901us/step - loss: 0.0435 - accuracy: 0.9859 - val_loss: 0.1080 - val_accuracy: 0.9675\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 908us/step - loss: 0.0394 - accuracy: 0.9873 - val_loss: 0.0945 - val_accuracy: 0.9732\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 1s 909us/step - loss: 0.0333 - accuracy: 0.9894 - val_loss: 0.0915 - val_accuracy: 0.9733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.3735 - accuracy: 0.8956 - val_loss: 0.1853 - val_accuracy: 0.9446\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.1646 - accuracy: 0.9521 - val_loss: 0.1380 - val_accuracy: 0.9592\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.1211 - accuracy: 0.9638 - val_loss: 0.1165 - val_accuracy: 0.9648\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0951 - accuracy: 0.9713 - val_loss: 0.1020 - val_accuracy: 0.9701\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0794 - accuracy: 0.9761 - val_loss: 0.1015 - val_accuracy: 0.9695\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0677 - accuracy: 0.9796 - val_loss: 0.0955 - val_accuracy: 0.9716\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0576 - accuracy: 0.9830 - val_loss: 0.0921 - val_accuracy: 0.9736\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0492 - accuracy: 0.9850 - val_loss: 0.0952 - val_accuracy: 0.9733\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0441 - accuracy: 0.9868 - val_loss: 0.0869 - val_accuracy: 0.9763\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0359 - accuracy: 0.9897 - val_loss: 0.0890 - val_accuracy: 0.9755\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Step 1: Preprocess the dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Experiment with different batch sizes\n",
    "batch_sizes = [16, 32, 64, 128]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # Step 2: Implement a simple feedforward neural network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # Step 3: Train the neural network without batch normalization\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(x_train.reshape(-1, 784), y_train, epochs=10, batch_size=batch_size, validation_data=(x_test.reshape(-1, 784), y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed533e",
   "metadata": {},
   "source": [
    "#### In the code above, we iterate over different batch sizes and train the model without using batch normalization. By experimenting with different batch sizes (e.g., 16, 32, 64, 128), you can observe the effect on training dynamics and model performance. You can analyze the convergence speed, stability, and overall accuracy for each batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d58f61",
   "metadata": {},
   "source": [
    "#### Advantages of batch normalization:\n",
    "\n",
    "1) Improved convergence: Batch normalization helps to stabilize and speed up the training process by reducing the internal covariate shift problem. It normalizes the inputs of each layer, ensuring that the mean activation is close to zero and the variance is close to one. This allows for more efficient weight updates during backpropagation and faster convergence.\n",
    "\n",
    "2) Reduced sensitivity to weight initialization: Batch normalization reduces the dependence of the model's performance on the initial values of the weights. It enables the use of higher learning rates without causing the network to diverge or get stuck in local minima.\n",
    "\n",
    "3) Regularization effect: Batch normalization acts as a form of regularization by adding a small amount of noise to the network during training. This noise helps to prevent overfitting and improve generalization performance.\n",
    "\n",
    "\n",
    "\n",
    "#### Limitations and considerations:\n",
    "\n",
    "1) Increased computational cost: Batch normalization introduces additional computations during both the forward and backward passes of the network, which can slightly increase the training time.\n",
    "\n",
    "2) Dependency on batch size: The effectiveness of batch normalization can vary with different batch sizes. In some cases, very small batch sizes (e.g., 1 or 2) may lead to unstable or inaccurate results. It is generally recommended to use reasonably sized mini-batches (e.g., 32 or 64) to leverage the benefits of batch normalization effectively.\n",
    "\n",
    "3) Influence on model performance: While batch normalization can often improve model performance, it may not always be beneficial or necessary. In certain cases, for small networks or datasets, batch normalization might not provide significant advantages and can even hinder performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3366d3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
