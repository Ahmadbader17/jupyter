{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f86f89-0898-4279-80a7-b4f6a9d570f5",
   "metadata": {},
   "source": [
    "# Answer 1\n",
    "\n",
    "Lasso Regression, also known as L1 regularization, is a linear regression technique that adds a penalty term to the cost function to encourage the model to have fewer non-zero coefficients. This encourages the model to select only the most important features and can help prevent overfitting.\n",
    "\n",
    "Compared to other regression techniques like Ridge Regression (L2 regularization) and Ordinary Least Squares (OLS), Lasso Regression differs in the penalty term it adds to the cost function. While Ridge Regression adds the squared sum of the coefficients, Lasso Regression adds the absolute sum of the coefficients. This results in a sparser model with fewer non-zero coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05186e6-7934-4852-abc2-6f6001336b25",
   "metadata": {},
   "source": [
    "# Answer 2\n",
    "\n",
    "The main advantage of using Lasso Regression in feature selection is that it can automatically select the most important features and eliminate the irrelevant or redundant ones. This is particularly useful in high-dimensional data sets with many features, where selecting the right subset of features can improve the model's accuracy and interpretability.\n",
    "\n",
    "Lasso Regression achieves feature selection by shrinking the coefficients of the irrelevant or redundant features to zero. This is because the penalty term in the cost function encourages the model to have fewer non-zero coefficients. The resulting sparse model can improve model performance and reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cfdeb7-0ccf-4c7c-b1a0-bc19cbe555da",
   "metadata": {},
   "source": [
    "# Answer 3\n",
    "\n",
    "The coefficients of a Lasso Regression model can be interpreted as the impact of each feature on the predicted output. A positive coefficient means that the feature has a positive effect on the output, while a negative coefficient means that the feature has a negative effect on the output.\n",
    "\n",
    "However, it is important to note that the coefficients in a Lasso Regression model can be biased and difficult to interpret when the features are highly correlated. This is because Lasso Regression tends to select only one feature from a group of highly correlated features, leading to unstable and biased coefficient estimates.\n",
    "\n",
    "To address this issue, researchers have developed alternative techniques such as Elastic Net Regression, which combines both L1 and L2 regularization to achieve better balance between feature selection and coefficient stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a613ed-046a-4b32-bada-5b1a7b6f6e64",
   "metadata": {},
   "source": [
    "# Answer 4\n",
    "\n",
    "The main tuning parameter in Lasso Regression is the regularization parameter, also known as the lambda parameter. This parameter controls the strength of the penalty term added to the cost function and determines the sparsity of the resulting model.\n",
    "\n",
    "A higher lambda value leads to a sparser model with fewer non-zero coefficients, which can help prevent overfitting and improve model interpretability. However, setting the lambda value too high can also lead to underfitting and decreased model performance. On the other hand, setting the lambda value too low can lead to overfitting and decreased model generalization.\n",
    "\n",
    "Another tuning parameter that can be adjusted in Lasso Regression is the intercept term, which represents the value of the predicted output when all the features have a value of zero. The intercept term can be adjusted to improve model accuracy and fit the data better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3434ef38-82d1-481e-ba34-270da660f7cf",
   "metadata": {},
   "source": [
    "# Answer 5\n",
    "\n",
    "Lasso Regression is a linear regression technique and is designed to model linear relationships between the features and the output. However, it can be extended to non-linear regression problems by including polynomial features or other non-linear transformations of the features.\n",
    "\n",
    "By including polynomial features, Lasso Regression can model non-linear relationships between the features and the output. However, adding too many polynomial features can lead to overfitting and decreased model performance. Therefore, it is important to carefully select the appropriate degree of polynomial features to balance model complexity and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b8ec5d-f33e-48f2-83f5-ca2971ce683e",
   "metadata": {},
   "source": [
    "# Answer 6\n",
    "\n",
    "Ridge Regression and Lasso Regression are two popular linear regression techniques that add a penalty term to the cost function to improve model performance and prevent overfitting.\n",
    "\n",
    "The main difference between Ridge Regression and Lasso Regression is the type of penalty term they use. Ridge Regression adds the squared sum of the coefficients (L2 regularization) to the cost function, while Lasso Regression adds the absolute sum of the coefficients (L1 regularization).\n",
    "\n",
    "As a result, Ridge Regression tends to produce models with smaller coefficients and can handle highly correlated features better than Lasso Regression. On the other hand, Lasso Regression tends to produce sparser models with fewer non-zero coefficients and can perform feature selection automatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be4a250-781b-42a1-8287-7dcea2eabfaf",
   "metadata": {},
   "source": [
    "# Answer 7\n",
    "\n",
    "Lasso Regression can handle multicollinearity in the input features to some extent, but it may not perform as well as Ridge Regression in this scenario. Multicollinearity occurs when two or more input features are highly correlated with each other, which can lead to unstable and biased coefficient estimates in linear regression models.\n",
    "\n",
    "In Lasso Regression, the penalty term encourages the model to select only one feature from a group of highly correlated features, leading to a sparse model with fewer non-zero coefficients. However, this may not be sufficient to fully address multicollinearity, as it may not be clear which of the highly correlated features is the most important for the model.\n",
    "\n",
    "Ridge Regression, on the other hand, can handle multicollinearity more effectively by shrinking the coefficients of all the correlated features together. This helps to reduce the impact of multicollinearity on the model and can improve the stability of the coefficient estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896cd00f-ee41-4f28-b4cd-c11a20db87cd",
   "metadata": {},
   "source": [
    "# Answer "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
