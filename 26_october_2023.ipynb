{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71034fc0-1b2c-452b-9c61-29f9b7cd7476",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fbede0-309a-4a7b-b5e0-d0ebfbe4a026",
   "metadata": {},
   "source": [
    "## Answer 1\n",
    "\n",
    "### Core Components of the Hadoop Ecosystem:\n",
    "\n",
    "Hadoop Distributed File System (HDFS): HDFS is the storage component of Hadoop. It divides large files into smaller blocks and stores multiple copies of these blocks across different nodes for fault tolerance.\n",
    "\n",
    "MapReduce: MapReduce is the processing component that allows distributed processing of large datasets across a Hadoop cluster. It consists of a Map phase for processing and a Reduce phase for summarization.\n",
    "\n",
    "YARN (Yet Another Resource Negotiator): YARN is the resource management layer that handles resource allocation and job scheduling in a Hadoop cluster. It decouples the processing engine (MapReduce, Spark, etc.) from resource management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c75c60-7ffb-49f2-b6e0-6f7e4d4e0269",
   "metadata": {},
   "source": [
    "## Answer 2\n",
    "\n",
    "### Hadoop Distributed File System (HDFS):\n",
    "\n",
    "HDFS is designed for reliability and fault tolerance. It stores data by breaking it into blocks (typically 128 MB or 256 MB in size) and distributes these blocks across the cluster. Multiple copies of each block are stored on different nodes, ensuring fault tolerance. If a node fails, the data can be retrieved from other nodes holding copies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0432baa-d5cd-4933-8c13-d115b16044cb",
   "metadata": {},
   "source": [
    "## Answer 3\n",
    "\n",
    "### MapReduce Framework:\n",
    "\n",
    "Map Phase: Input data is divided into smaller chunks, and the Map function processes each chunk independently, emitting key-value pairs.\n",
    "\n",
    "Shuffle and Sort Phase: The framework shuffles and sorts the output of the Map phase, ensuring that all values associated with a particular key are grouped together.\n",
    "\n",
    "Reduce Phase: The Reduce function processes the sorted and shuffled data, producing the final output.\n",
    "\n",
    "Example: Word Count\n",
    "\n",
    "Map Phase: Count occurrences of each word in a document.\n",
    "Shuffle and Sort Phase: Group occurrences of each word together.\n",
    "Reduce Phase: Sum up the counts for each word.\n",
    "Advantages of MapReduce:\n",
    "\n",
    "Scalability: Scales horizontally to handle large datasets.\n",
    "Fault Tolerance: Redundant storage and task re-execution ensure fault tolerance.\n",
    "Limitations:\n",
    "\n",
    "Batch Processing: Designed for batch processing and may not be suitable for real-time processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d40b1-6070-43cd-a58b-acd9795ec9ef",
   "metadata": {},
   "source": [
    "## Answer 4\n",
    "\n",
    "### YARN (Yet Another Resource Negotiator):\n",
    "\n",
    "YARN manages resources and schedules applications. It allows different processing engines like MapReduce, Spark, and others to share and efficiently utilize cluster resources. Compared to Hadoop 1.x, YARN provides a more flexible and scalable architecture, allowing diverse workloads to run concurrently on the same cluster.\n",
    "\n",
    "Benefits of YARN:\n",
    "\n",
    "Flexibility: Supports various processing engines.\n",
    "Improved Resource Management: Efficient utilization of resources.\n",
    "Scalability: Scales better than Hadoop 1.x for diverse workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9378d57-00f9-4f8e-812d-37f788dec251",
   "metadata": {},
   "source": [
    "## Answer 5\n",
    "\n",
    "### Popular Hadoop Ecosystem Components:\n",
    "\n",
    "HBase: A NoSQL database for real-time read/write access to large datasets.\n",
    "\n",
    "Hive: Provides a data warehousing and SQL-like interface for querying data stored in Hadoop.\n",
    "\n",
    "Pig: A high-level scripting language for creating MapReduce programs.\n",
    "\n",
    "Spark: A fast and general-purpose cluster computing system for big data processing.\n",
    "\n",
    "Integration Example: HBase\n",
    "HBase can be integrated into the Hadoop ecosystem to provide real-time access to large datasets. It stores data in HDFS and allows for low-latency reads and writes. It is suitable for use cases requiring random, real-time access to data, such as in applications involving time-series data or online analytical processing (OLAP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadd3c0a-6a3f-41a7-b5e2-6b9d5343746c",
   "metadata": {},
   "source": [
    "## Answer 6\n",
    "\n",
    "### Key Differences Between Apache Spark and Hadoop MapReduce:\n",
    "\n",
    "Processing Model:\n",
    "\n",
    "MapReduce: Batch-oriented, two-stage processing model.\n",
    "Spark: Supports batch processing, interactive queries, streaming, and iterative algorithms.\n",
    "Performance:\n",
    "\n",
    "MapReduce: Writes intermediate data to disk after each stage, leading to slower performance.\n",
    "Spark: In-memory computation and lazy evaluation result in faster processing.\n",
    "Ease of Use:\n",
    "\n",
    "MapReduce: Requires more lines of code for complex algorithms.\n",
    "Spark: Offers high-level APIs in Java, Scala, Python, and R, making it more user-friendly.\n",
    "Data Processing:\n",
    "\n",
    "MapReduce: Limited to batch processing.\n",
    "Spark: Supports batch, interactive, streaming, and machine learning workloads.\n",
    "Data Sharing:\n",
    "\n",
    "MapReduce: Communicates through disk, causing I/O overhead.\n",
    "Spark: Utilizes in-memory data sharing, reducing communication overhead.\n",
    "Fault Tolerance:\n",
    "\n",
    "MapReduce: Achieves fault tolerance through data replication.\n",
    "Spark: Utilizes lineage information to reconstruct lost data, minimizing data replication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48622e-3e6a-4fb5-8a76-2bb8a0b07b64",
   "metadata": {},
   "source": [
    "## Answer 7\n",
    "\n",
    "### Spark Application for Word Count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47577cb7-96a9-4b67-bcd5-5fe3329ab9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Set up Spark configuration and context\n",
    "conf = SparkConf().setAppName(\"WordCountApp\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Read input text file\n",
    "input_file = \"path/to/your/textfile.txt\"\n",
    "text_file = sc.textFile(input_file)\n",
    "\n",
    "# Perform word count\n",
    "word_counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "                      .map(lambda word: (word, 1)) \\\n",
    "                      .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Get the top 10 most frequent words\n",
    "top_words = word_counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "# Print the result\n",
    "for word, count in top_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Stop the Spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d108924-38af-4628-be72-8fe8f190874a",
   "metadata": {},
   "source": [
    "#### Key Components and Steps:\n",
    "\n",
    "Spark Configuration: Set up a Spark configuration and create a Spark context.\n",
    "Read Data: Use textFile to read the input text file.\n",
    "Word Count: Use flatMap, map, and reduceByKey to perform word count.\n",
    "Top Words: Use takeOrdered to retrieve the top 10 words based on count.\n",
    "Print Result: Display the results.\n",
    "Stop Spark Context: Terminate the Spark context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f7812d-4383-4df7-aba6-80364440a2b6",
   "metadata": {},
   "source": [
    "## Answer 8\n",
    "\n",
    "### Spark RDD Operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44762cbf-6aca-4ad8-b2b0-7c0b6c7911df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Set up Spark configuration and context\n",
    "conf = SparkConf().setAppName(\"RDDOperations\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Load dataset (replace 'your_dataset_path' with the actual path)\n",
    "data = sc.textFile('your_dataset_path')\n",
    "\n",
    "# a. Filter data to select only rows that meet specific criteria\n",
    "filtered_data = data.filter(lambda line: \"specific_criteria\" in line)\n",
    "\n",
    "# b. Map a transformation to modify a specific column in the dataset\n",
    "mapped_data = data.map(lambda line: line.replace(\"old_value\", \"new_value\"))\n",
    "\n",
    "# c. Reduce the dataset to calculate a meaningful aggregation (e.g., sum, average)\n",
    "numeric_data = mapped_data.map(lambda line: int(line.split(',')[1]))  # Assuming the second column is numeric\n",
    "sum_result = numeric_data.reduce(lambda x, y: x + y)\n",
    "average_result = sum_result / numeric_data.count()\n",
    "\n",
    "# Print results\n",
    "print(\"Filtered Data:\")\n",
    "print(filtered_data.collect())\n",
    "print(\"\\nMapped Data:\")\n",
    "print(mapped_data.collect())\n",
    "print(\"\\nSum Result:\", sum_result)\n",
    "print(\"Average Result:\", average_result)\n",
    "\n",
    "# Stop the Spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33ddf36-c96f-474f-a030-2bf88156d381",
   "metadata": {},
   "source": [
    "The above example demonstrates how to filter data based on specific criteria, map a transformation to modify a column, and reduce the dataset to calculate the sum and average of a numeric column. Adjust the operations based on your dataset and requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9001c560-46bf-42c7-a2ff-f6897be8f836",
   "metadata": {},
   "source": [
    "## Answer 9\n",
    "\n",
    "### Spark DataFrame Operations:\n",
    "\n",
    "#### For this example, let's assume we have a CSV dataset with the following structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fe78c0-1e04-425e-8af3-1045b4ac41af",
   "metadata": {},
   "source": [
    "ID,Name,Salary,Department\n",
    "\n",
    "1,John,50000,IT\n",
    "\n",
    "2,Jane,60000,HR\n",
    "\n",
    "3,Bob,55000,IT\n",
    "\n",
    "4,Alice,70000,Finance\n",
    "\n",
    "5,Charlie,45000,HR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe536245-f830-445a-b1da-855e83de5c0e",
   "metadata": {},
   "source": [
    "#### a. Select Specific Columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e0b405-ebec-4e20-bf04-893c23d99623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate()\n",
    "\n",
    "# Load the CSV dataset\n",
    "df = spark.read.csv(\"path/to/your/dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Select specific columns\n",
    "selected_columns = df.select(\"Name\", \"Salary\")\n",
    "selected_columns.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace8058d-09c7-4dcb-96ed-a41b21f65be7",
   "metadata": {},
   "source": [
    "#### b. Filter Rows Based on Conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97e5750-1d08-4afd-aa08-9b0e14528b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on a condition (e.g., Salary greater than 55000)\n",
    "filtered_data = df.filter(df[\"Salary\"] > 55000)\n",
    "filtered_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d4b25-3690-4f91-9697-ced3292a31d8",
   "metadata": {},
   "source": [
    "#### c. Group Data and Calculate Aggregations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ed1f6f-8819-4321-9564-e0476c600722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by the Department column and calculate average salary\n",
    "grouped_data = df.groupBy(\"Department\").agg({\"Salary\": \"avg\"})\n",
    "grouped_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b8e809-3eed-40a1-b600-bc57a59d863a",
   "metadata": {},
   "source": [
    "#### d. Join Two DataFrames Based on a Common Key:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f18a76d-ef52-4ba3-8e8e-5a8f88504207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second DataFrame for demonstration purposes\n",
    "df2 = spark.read.csv(\"path/to/your/second_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Join the two DataFrames based on the common key \"ID\"\n",
    "joined_data = df.join(df2, df[\"ID\"] == df2[\"ID\"], \"inner\")\n",
    "joined_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db464c9f-ce0a-4db5-ae32-d8acc7a897e3",
   "metadata": {},
   "source": [
    "## Answer 10\n",
    "\n",
    "### Spark Streaming Application:\n",
    "\n",
    "#### For this example, let's assume you have a Spark cluster and Apache Kafka running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b149d728-3bff-4c04-9829-4c33e2b71b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"SparkStreamingApp\").getOrCreate()\n",
    "\n",
    "# Set up the StreamingContext with a batch interval of 5 seconds\n",
    "ssc = StreamingContext(spark.sparkContext, 5)\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_params = {\"bootstrap.servers\": \"your_kafka_broker\", \"group.id\": \"spark-streaming-group\"}\n",
    "\n",
    "# Create a DStream that connects to Kafka and ingests data in micro-batches\n",
    "stream = KafkaUtils.createDirectStream(ssc, [\"your_kafka_topic\"], kafka_params)\n",
    "\n",
    "# Apply a transformation to the streaming data (e.g., word count)\n",
    "transformed_data = stream.map(lambda x: x[1].split(\" \")) \\\n",
    "                        .flatMap(lambda words: [(word, 1) for word in words]) \\\n",
    "                        .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Output the processed data to a sink (e.g., print to console)\n",
    "transformed_data.pprint()\n",
    "\n",
    "# Start the StreamingContext\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6553ec-05f6-4126-8913-102ff48be16a",
   "metadata": {},
   "source": [
    "## Answer 11\n",
    "\n",
    "### Fundamental Concepts of Apache Kafka:\n",
    "\n",
    "Apache Kafka is a distributed streaming platform that aims to solve the problems of data integration, real-time data processing, and event-driven architectures. It provides fault tolerance, scalability, and durability in handling large-scale, real-time data streams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d171e34-8d34-425a-ae84-5177749e7391",
   "metadata": {},
   "source": [
    "## Answer 13\n",
    "\n",
    "### Producing and Consuming Data in Kafka:\n",
    "\n",
    "#### For Python, you can use the confluent_kafka library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8964400e-d834-4b6b-86dd-eb51c204d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer, Consumer, KafkaError\n",
    "\n",
    "# Produce data to a Kafka topic\n",
    "producer = Producer({'bootstrap.servers': 'your_kafka_broker'})\n",
    "\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print('Message delivery failed: {}'.format(err))\n",
    "    else:\n",
    "        print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))\n",
    "\n",
    "producer.produce('your_kafka_topic', key='key', value='value', callback=delivery_report)\n",
    "\n",
    "# Consume data from a Kafka topic\n",
    "consumer = Consumer({\n",
    "    'bootstrap.servers': 'your_kafka_broker',\n",
    "    'group.id': 'your_consumer_group',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "})\n",
    "\n",
    "consumer.subscribe(['your_kafka_topic'])\n",
    "\n",
    "while True:\n",
    "    msg = consumer.poll(1.0)\n",
    "\n",
    "    if msg is None:\n",
    "        continue\n",
    "    if msg.error():\n",
    "        if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "            continue\n",
    "        else:\n",
    "            print(msg.error())\n",
    "            break\n",
    "\n",
    "    print('Received message: {}'.format(msg.value().decode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2bd6cd-005f-49a0-b9c1-17400fb3b209",
   "metadata": {},
   "source": [
    "## Answer 14\n",
    "\n",
    "### Data Retention and Partitioning in Kafka:\n",
    "\n",
    "Data Retention: Configured by setting the log.retention.hours property. It determines how long Kafka retains messages.\n",
    "Data Partitioning: Allows parallel processing and scalability. Topics can be partitioned, and each partition is processed independently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e705de-df5f-46e9-b9aa-9a3e92a5471f",
   "metadata": {},
   "source": [
    "## Answer 15\n",
    "\n",
    "### Real-World Use Cases of Apache Kafka:\n",
    "\n",
    "Log Aggregation: Centralized log collection for distributed systems.\n",
    "Data Integration: Connecting various data sources and sinks.\n",
    "Event Sourcing: Capturing and storing events as a source of truth.\n",
    "Stream Processing: Real-time analytics and monitoring.\n",
    "Messaging System: Reliable, scalable communication between microservices.\n",
    "Kafka is preferred for its durability, scalability, and ability to handle high-throughput, fault-tolerant data streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da3c2a-a55e-44d3-9aa2-d502b38dda66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
