{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Answer 1\n",
    "\n",
    "R-squared is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variable(s) in a linear regression model. It is also known as the coefficient of determination. R-squared is a value between 0 and 1, where a value of 1 indicates that all the variation in the dependent variable is explained by the independent variable(s) in the model, and a value of 0 indicates that none of the variation in the dependent variable is explained by the independent variable(s) in the model.\n",
    "\n",
    "R-squared is calculated by taking the ratio of the explained variance (sum of squares of the regression, SSR) to the total variance (sum of squares of the residuals, SSE), as follows:\n",
    "\n",
    "R-squared = 1 - (SSE / SST)\n",
    "\n",
    "where SST is the total sum of squares, which is the sum of squares of the residuals plus the sum of squares of the regression (SSR + SSE).\n",
    "\n",
    "R-squared is commonly used as a goodness-of-fit measure in linear regression models. A high R-squared value indicates that the model fits the data well and that the independent variable(s) are good predictors of the dependent variable."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Answer 2\n",
    "\n",
    "Adjusted R-squared is a modified version of R-squared that takes into account the number of independent variables in the model. Regular R-squared may increase as more independent variables are added to the model, even if those variables do not have a significant effect on the dependent variable. This is known as overfitting. Adjusted R-squared penalizes for overfitting by adjusting for the number of independent variables in the model.\n",
    "\n",
    "Adjusted R-squared is calculated as follows:\n",
    "\n",
    "Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - k - 1)]\n",
    "\n",
    "where n is the sample size and k is the number of independent variables in the model.\n",
    "\n",
    "Adjusted R-squared will always be lower than regular R-squared, as it penalizes for the inclusion of unnecessary independent variables. A higher adjusted R-squared value indicates a better fit of the model, as it takes into account the number of independent variables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Answer 3\n",
    "\n",
    "It is more appropriate to use adjusted R-squared when comparing models with different numbers of independent variables. Regular R-squared can increase as more independent variables are added to the model, even if those variables do not have a significant effect on the dependent variable. Adjusted R-squared takes into account the number of independent variables, and therefore, it provides a better measure of the goodness of fit of the model. In general, adjusted R-squared is a better measure of model fit when comparing models with different numbers of independent variables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Answer 4\n",
    "\n",
    "In regression analysis, RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are common evaluation metrics used to measure the accuracy of the model's predictions.\n",
    "\n",
    "MSE is calculated by taking the average of the squared differences between the predicted values and the actual values:\n",
    "\n",
    "MSE = 1/n * sum((yi - y^i)^2)\n",
    "\n",
    "where yi is the actual value, y^i is the predicted value, and n is the number of observations.\n",
    "\n",
    "RMSE is calculated by taking the square root of the MSE:\n",
    "\n",
    "RMSE = sqrt(MSE)\n",
    "\n",
    "MAE is calculated by taking the average of the absolute differences between the predicted values and the actual values:\n",
    "\n",
    "MAE = 1/n * sum(|yi - y^i|)\n",
    "\n",
    "where yi is the actual value, y^i is the predicted value, and n is the number of observations.\n",
    "\n",
    "MSE, RMSE, and MAE are all measures of the average error of the model's predictions. RMSE and MSE give more weight to larger errors, while MAE treats all errors equally."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Answer 5\n",
    "\n",
    "The advantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis include:\n",
    "\n",
    "These metrics provide a quantitative measure of the accuracy of the model's predictions, making it easy to compare the performance of different models.\n",
    "\n",
    "They are easy to understand and interpret, making them accessible to a wide range of stakeholders.\n",
    "\n",
    "They can be used to identify which variables in the model are contributing the most to the error.\n",
    "\n",
    "\n",
    "The disadvantages of using these metrics include:\n",
    "\n",
    "They do not provide information on the direction of the error. A model with a large negative error may be equally bad as a \n",
    "model with a large positive error, but these metrics treat them as the same.\n",
    "\n",
    "They do not provide information on how the errors are distributed across the range of the dependent variable. A model may perform well in one area of the data but poorly in another, and these metrics may not capture that.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Answer 6\n",
    "\n",
    "Lasso regularization is a method used to reduce the complexity of a linear regression model by penalizing the model for having too many non-zero coefficients. This is achieved by adding a penalty term to the cost function of the model, which is proportional to the absolute value of the coefficients. This penalty term encourages some coefficients to become zero, effectively removing the corresponding variables from the model.\n",
    "\n",
    "Lasso regularization differs from Ridge regularization in that it uses an L1 penalty term, which encourages sparsity in the model (i.e., some coefficients are exactly zero), while Ridge regularization uses an L2 penalty term, which shrinks all coefficients towards zero but does not encourage sparsity.\n",
    "\n",
    "Lasso regularization is more appropriate when the number of independent variables is large and there is a suspicion that only a few of them are truly important in predicting the dependent variable. Ridge regularization is more appropriate when there is multicollinearity between the independent variables, as it can reduce the variance of the coefficient estimates."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Answer 7\n",
    "\n",
    "Regularized linear models, such as Lasso and Ridge regression, help to prevent overfitting in machine learning by reducing the complexity of the model. Regularization achieves this by adding a penalty term to the loss function that the model is trying to minimize. This penalty term acts as a constraint on the coefficients of the model, preventing them from becoming too large and potentially overfitting the training data.\n",
    "\n",
    "For example, consider a linear regression model with a large number of features. Without regularization, the model may become too complex and overfit the training data, leading to poor generalization performance on unseen data. However, by adding a regularization term to the cost function, the model can be constrained to only include the most important features, effectively reducing its complexity and improving its ability to generalize to new data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Answer 8\n",
    "\n",
    "Regularized linear models have some limitations that may make them less suitable for certain regression analysis tasks.\n",
    "\n",
    "Firstly, regularization assumes that the underlying relationship between the independent and dependent variables is linear. If this assumption is not met, regularized linear models may not perform well.\n",
    "\n",
    "Secondly, regularized linear models may not be effective when the relationship between the independent and dependent variables is highly non-linear. In such cases, other machine learning models, such as decision trees or neural networks, may be more appropriate.\n",
    "\n",
    "Thirdly, regularized linear models can be sensitive to the choice of regularization parameter. Selecting the appropriate value of the regularization parameter can be challenging, and if the wrong value is chosen, the model may still overfit or underfit the data.\n",
    "\n",
    "Lastly, regularized linear models may not be well-suited to dealing with high-dimensional data. As the number of features increases, the computational cost of fitting the model can become prohibitively expensive. In such cases, other techniques such as dimensionality reduction may be more appropriate."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Answer 9\n",
    "\n",
    "The choice of which model is better depends on the specific needs of the task at hand. If the goal is to minimize the average difference between the predicted and actual values (MAE), then Model B would be the better performer as it has a lower MAE of 8. However, if the goal is to minimize the overall deviation between the predicted and actual values (RMSE), then Model A would be the better performer as it has a lower RMSE of 10.\n",
    "\n",
    "There are limitations to using these metrics as a sole measure of model performance. RMSE penalizes larger errors more than smaller errors, which may be inappropriate if the task is sensitive to smaller errors. On the other hand, MAE treats all errors equally, which may be inappropriate if larger errors are more critical to the task."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Answer 10\n",
    "\n",
    "Again, the choice of which model is better depends on the specific needs of the task at hand. However, in general, the choice of regularization method depends on the nature of the problem and the type of features being used. If the goal is to select a small subset of important features, Lasso regularization may be more appropriate. On the other hand, if the goal is to reduce the impact of all features without completely eliminating any, Ridge regularization may be more appropriate.\n",
    "\n",
    "There are trade-offs and limitations to both regularization methods. Lasso regularization tends to produce sparse models, meaning it eliminates some of the features entirely, while Ridge regularization only reduces the impact of the features. Additionally, Lasso can have difficulty selecting a small subset of features when they are highly correlated, while Ridge may not be effective in dealing with large numbers of correlated features. The choice of the regularization parameter for either method can also be challenging and requires careful tuning to obtain optimal performance."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}