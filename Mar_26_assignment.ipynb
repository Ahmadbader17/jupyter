{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Answer 1\n",
    "\n",
    "Simple linear regression involves finding a linear relationship between two variables, where one variable (the independent variable) is used to predict the other variable (the dependent variable). It involves one independent variable and one dependent variable. On the other hand, multiple linear regression involves finding the linear relationship between a dependent variable and multiple independent variables. It involves two or more independent variables and one dependent variable.\n",
    "\n",
    "Example of Simple Linear Regression: Predicting the price of a house based on its size. Here, the size of the house is the independent variable, and the price of the house is the dependent variable.\n",
    "\n",
    "Example of Multiple Linear Regression: Predicting the salary of a person based on their age, education level, and work experience. Here, the age, education level, and work experience are the independent variables, and the salary is the dependent variable."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Answer 2\n",
    "\n",
    "Linear regression assumes that there is a linear relationship between the independent variables and the dependent variable, and the residuals (the difference between the predicted and actual values) follow a normal distribution. The following assumptions are necessary for linear regression:\n",
    "\n",
    "Linearity: The relationship between the independent variables and the dependent variable should be linear.\n",
    "Independence: The observations should be independent of each other.\n",
    "Homoscedasticity: The residuals should have equal variance across all values of the independent variable.\n",
    "Normality: The residuals should follow a normal distribution.\n",
    "No multicollinearity: The independent variables should not be highly correlated.\n",
    "To check whether these assumptions hold in a given dataset, we can use diagnostic plots like scatter plots, residual plots, and normal probability plots."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Answer 3\n",
    "\n",
    "In a linear regression model, the slope represents the change in the dependent variable for every unit change in the independent variable. The intercept represents the expected value of the dependent variable when the independent variable is zero.\n",
    "\n",
    "Example: Suppose we have a dataset of student scores in a test, and we want to predict the score based on the number of hours they studied. A linear regression model shows that the slope is 5 and the intercept is 50. This means that for every additional hour of study, we can expect the score to increase by 5 points. If a student did not study at all, we can expect their score to be 50."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Answer 4\n",
    "\n",
    "Gradient descent is an iterative optimization algorithm used to find the optimal solution for a given problem. It is used in machine learning to minimize the cost function (i.e., the error between predicted and actual values) by updating the model's parameters (coefficients) iteratively. The algorithm works by calculating the gradient of the cost function with respect to each parameter and updating the parameter in the opposite direction of the gradient. By repeating this process, the algorithm approaches the optimal solution."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Answer 5\n",
    "\n",
    "Multiple linear regression is a statistical technique that models the relationship between a dependent variable and multiple independent variables. The model is expressed as:\n",
    "\n",
    "Y = b0 + b1X1 + b2X2 + ... + bn*Xn + ε\n",
    "\n",
    "where Y is the dependent variable, X1 to Xn are the independent variables, b0 is the intercept, and b1 to bn are the coefficients or slopes of the independent variables. In multiple linear regression, the model includes two or more independent variables, whereas in simple linear regression, there is only one independent variable."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Answer 6\n",
    "\n",
    "Multicollinearity in multiple linear regression refers to a situation where two or more independent variables in the model are highly correlated with each other. This can cause problems in the regression analysis as it becomes difficult to distinguish the effect of each independent variable on the dependent variable.\n",
    "\n",
    "To detect multicollinearity, we can use correlation matrix or Variance Inflation Factor (VIF) value. If the correlation coefficient between two independent variables is high, it indicates that there might be multicollinearity. Similarly, if the VIF value of an independent variable is high, it indicates that there might be multicollinearity between that variable and other independent variables.\n",
    "\n",
    "To address multicollinearity, we can either remove one of the highly correlated independent variables or use dimensionality reduction techniques such as Principal Component Analysis (PCA) to reduce the number of independent variables while retaining most of the information."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Answer 7\n",
    "\n",
    "\n",
    "Polynomial regression is a form of regression analysis where the relationship between the dependent variable and independent variable(s) is modeled as an nth degree polynomial. Unlike linear regression, which models the relationship between the dependent variable and independent variable(s) as a straight line, polynomial regression allows for more complex relationships between the variables.\n",
    "\n",
    "The polynomial regression equation is expressed as:\n",
    "\n",
    "y = b0 + b1x + b2x^2 + ... + bn*x^n + ε\n",
    "\n",
    "where y is the dependent variable, x is the independent variable, b0 to bn are the coefficients or slopes of the independent variables, n is the degree of the polynomial, and ε is the error term."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Answer 8\n",
    "\n",
    "The advantages of polynomial regression over linear regression are that it can capture more complex relationships between the dependent variable and independent variable(s) and can provide better accuracy for certain datasets. Additionally, polynomial regression can be useful for making predictions outside the range of the original data.\n",
    "\n",
    "The disadvantages of polynomial regression are that it can easily overfit the data, especially with higher degree polynomials, which can lead to poor generalization performance. Furthermore, interpreting the coefficients in a polynomial regression model can be more difficult than in a linear regression model.\n",
    "\n",
    "Polynomial regression is useful in situations where the relationship between the dependent variable and independent variable(s) is non-linear or complex, and simple linear regression is not sufficient. However, it is important to carefully choose the degree of the polynomial to prevent overfitting, and to validate the model's performance on new data. Additionally, if the relationship between the variables is not well understood, it may be better to use a simpler model like linear regression."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}