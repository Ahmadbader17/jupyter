{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1240fcdb-c7f9-4a62-addb-5fe5cf66178e",
   "metadata": {},
   "source": [
    "# Answer 1\n",
    "\n",
    "Polynomial functions and kernel functions are related in the sense that polynomial functions can be used as a basis for kernel functions in machine learning algorithms. A kernel function is a mathematical function that maps data points from their original space to a new feature space, where they can be more easily separated or classified. The polynomial function is one of the common kernel functions used in Support Vector Machines (SVM) algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070e7b41-0b9d-4e6e-b8ed-98270059ed04",
   "metadata": {},
   "source": [
    "# Answer 2\n",
    "\n",
    "We can implement an SVM with a polynomial kernel in Python using Scikit-learn library by following these steps:\n",
    "\n",
    "1) Import the necessary packages: from sklearn import datasets and from sklearn.svm import SVC\n",
    "\n",
    "2) Load the data using the datasets.load_iris() method\n",
    "\n",
    "3) Define the SVM model with the polynomial kernel using SVC(kernel='poly', degree=3, C=1.0)\n",
    "\n",
    "4) Train the model with the data using the .fit() method\n",
    "\n",
    "5) Predict the outcomes of the model using the .predict() method\n",
    "\n",
    "Here is an example code snippet for implementing SVM with a polynomial kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "457da21d-b599-42ce-b463-3a36bdfa4e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1\n",
      " 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define the SVM model with the polynomial kernel\n",
    "svm_poly = SVC(kernel='poly', degree=3, C=1.0)\n",
    "\n",
    "# Train the model\n",
    "svm_poly.fit(X, y)\n",
    "\n",
    "# Predict the outcomes\n",
    "y_pred = svm_poly.predict(X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8619ded-257a-406a-8492-1eedf4373f70",
   "metadata": {},
   "source": [
    "# Answer 3\n",
    "\n",
    "The value of epsilon is a hyperparameter in Support Vector Regression (SVR) that controls the size of the margin around the predicted function where no penalty is incurred. Increasing the value of epsilon will increase the size of the margin and allow more errors in the predictions. As a result, increasing the value of epsilon will decrease the number of support vectors in SVR. However, a very large value of epsilon may result in underfitting, while a very small value may result in overfitting. So, it is important to choose the appropriate value of epsilon based on the data and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7279f3-d0b5-40d0-8126-9119bcfafc56",
   "metadata": {},
   "source": [
    "# Answer 4\n",
    "\n",
    "Support Vector Regression (SVR) is a supervised learning algorithm used for regression tasks. The performance of SVR is heavily dependent on the choice of kernel function, C parameter, epsilon parameter, and gamma parameter. Here is a brief explanation of each parameter and how it affects the performance of SVR:\n",
    "\n",
    "1) Kernel Function: The kernel function is a function that transforms the data from the input space to a higher-dimensional feature space, where it can be more easily separated or classified. The choice of kernel function determines the type of decision boundary that can be created between the data points. Some commonly used kernel functions in SVR are linear, polynomial, and radial basis function (RBF).\n",
    "\n",
    "2) C Parameter: The C parameter controls the trade-off between the size of the margin and the amount of error allowed in the training data. A smaller C value allows more errors in the training data, which can result in a wider margin and fewer support vectors. Conversely, a larger C value forces the model to classify all data points correctly, resulting in a smaller margin and more support vectors.\n",
    "\n",
    "3) Epsilon Parameter: The epsilon parameter determines the width of the epsilon-insensitive zone around the predicted function, where no penalty is incurred. This means that any prediction within the zone is considered correct, and the model does not incur any penalty.\n",
    "\n",
    "4) Gamma Parameter: The gamma parameter controls the shape of the decision boundary in the feature space. A small gamma value leads to a softer decision boundary, while a large gamma value leads to a more complex decision boundary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e5ce8f-f3d5-4b44-8eed-d9a4273eb2c4",
   "metadata": {},
   "source": [
    "# Answer 5\n",
    "\n",
    "Here is the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d198e-cfb9-495b-908b-1dc3b93ce030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries and load the dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets\n",
    "X = df.drop('target_variable', axis=1)\n",
    "y = df['target_variable']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Preprocess the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create an instance of the SVC classifier and train it on the training data\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the performance of the classifier using accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy score: {accuracy}\")\n",
    "\n",
    "# Step 7: Tune the hyperparameters of the SVC classifier using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Train the tuned classifier on the entire dataset\n",
    "svc_tuned = grid.best_estimator_\n",
    "svc_tuned.fit(X, y)\n",
    "\n",
    "# Step 9: Save the trained classifier to a file for future use\n",
    "joblib.dump(svc_tuned, 'svc_tuned.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
